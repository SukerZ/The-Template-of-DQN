'''å®šä¹‰è¶…å‚æ•°'''
max_steps = 100000
BUFFER_SIZE = 100000 
BATCH_SIZE = 32
GAMMA = 0.99
action_dim = 3
state_dim = 29
episode_count = 2000
max_steps = 100000
done = False
step = 0

if __name__ == "__main__":
    '''åˆå§‹åŒ–ï¼š
       1ã€ç¯å¢ƒ
       2ã€ç¥ç»ç½‘ç»œ
       3ã€ç»éªŒæ± 
    '''
    np.random.seed(1337)
    env = TorcsEnv(vision=False, throttle=True, gear_change=False)
    env.reset(relaunch=True)

    dqn = Brain(state_dim, action_dim)

    buff = ReplayBuffer(BUFFER_SIZE)

    for episode in range(episode_count):      #å³for ğ‘’ğ‘ğ‘–ğ‘ ğ‘œğ‘‘ğ‘’=1 to ğ‘‡ do
        ob = env.reset(relaunch = True)       #æ¯è½®episodeçš„åˆå§‹åŒ–
        s_t = np.hstack((ob.angle, ob.track, ob.trackPos, ob.speedX, ob.speedY,  ob.speedZ, ob.wheelSpinVel/100.0, ob.rpm) ).tolist()

        for j in range(max_steps):      #å³for ğ‘¡=1 to ğ‘‡ do
            loss = 0
            a_t = dqn.explore_policy(s_t)    #æ¢ç´¢ç­–ç•¥é€‰æ‹©åŠ¨ä½œa

            ob, r_t, done, info = env.step(a_t)   #æ‰§è¡ŒåŠ¨ä½œaï¼Œä»ç¯å¢ƒä¸­è·å¾—<s,a,r,s'>
            s_t1 = np.hstack((ob.angle, ob.track, ob.trackPos, ob.speedX, ob.speedY, ob.speedZ, ob.wheelSpinVel/100.0, ob.rpm)).tolist()
            buff.add(s_t, a_t, r_t, s_t1, done)   #å¹¶å­˜å…¥ç»éªŒæ± 
            
            batch = buff.getBatch(BATCH_SIZE)     #ä»ç»éªŒæ± ä¸­éšæœºæŠ½å–è®­ç»ƒæ ·æœ¬
            states = [e[0] for e in batch]
            actions = [e[1] for e in batch]
            rewards = [e[2] for e in batch]
            new_states = [e[3] for e in batch]
            dones = [e[4] for e in batch]
            y_t = np.zeros((len(batch) ) )

            new_a = brain.target_policy(new_states) #ç›®æ ‡ç­–ç•¥é€‰æ‹©argmaxQ(s',a')çš„a'
            target_q_values = brain.target_value(new_states, new_a)
            
            for j in range(len(batch) ):          #è®¡ç®—y_jçš„å€¼
                if dones[j]:
                    y_t[j] = rewards[j]
                else:
                    y_t[j] = rewards[j] + GAMMA * target_q_values[j].item()
           
            pre = brain.predict(states, actions)
            loss += brain.train(pre, y_t)

            s_t = s_t1    #çŠ¶æ€è½¬ç§»
            step += 1
            if done:
                break;

    env.reset()
    env.end()             #å…³é—­ç¯å¢ƒ
    print("Finish.")